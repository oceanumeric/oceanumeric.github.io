<!DOCTYPE html>
<html lang="en">
<head>
    <html lang="en">
  <head>
    <title>
      math
    </title>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name='author' content='Michael Wang Fei'>
    <meta name='keywords' content='
          machine learning,
          statistical machine learning,
          bayesian inference,
          statistics,
          computational statistics,
          linear algebra,
          numerical linear algebra,
          statistical software,
          deep learning,
          computer science,
          probability,
          math,
          mathematics,
          probabilistic reasoning
      '>
      <meta name='keywords' content=''>
      <link rel="stylesheet" href="/css/math.css">
      <link rel="stylesheet" href="/css/markdown.css">
      <link rel="stylesheet" href="/css/trac.css">
      <link rel="shortcut icon" type="image/png" href="/images/favicon.png">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
      <!-- The loading of KaTeX is deferred to speed up page rendering -->
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
      <link href='https://fonts.googleapis.com/css?family=Fira Sans' rel='stylesheet'>
      <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>math</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="math" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://oceanumeric.github.io//math/" />
<meta property="og:url" content="https://oceanumeric.github.io//math/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="math" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"math","url":"https://oceanumeric.github.io//math/"}</script>
<!-- End Jekyll SEO tag -->

  </head>
</html>


</head>

<body>
    <!DOCTYPE html>
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/math'>Math</a></li>
        <li><a href='/tags'>Tags</a></li>
    </ul>
</div>
</html>

    <div id='blog' class='wrap'>
        <div id='intro'>
            "Hello World!"
        </div>
        <div id='posts' class='section'>
            
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/04/svd-to-pca">
                            
                            From SVD to PCA
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        03 April 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    The applications of SVD are manifold. In this post, we will focus on the application of SVD to PCA, which is a great tool for dimensionality reduction.
                    
                </p>
                <span class='hidden'>1</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/04/QR-factorization">
                            
                            QR Factorization
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        02 April 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    A QR factorization is a factorization of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. This kind of decomposition is useful in solving linear least squares problems and in the eigendecomposition of a matrix, which shows the structure of the matrix in terms of its eigenvalues and eigenvectors.
                    
                </p>
                <span class='hidden'>2</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/04/solving-linear-systems">
                            
                            Solving Linear Systems
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        01 April 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Linear systems of equations are the bread and butter of numerical linear algebra. Solving them is at the core of many machine learning algorithms and engineering applications.
                    
                </p>
                <span class='hidden'>3</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/04/floating-point-arithmetic">
                            
                            Floating-Point Arithmetic
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        01 April 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Floating-Point arithmetic is a way of representing real numbers in a computer. It is a way of representing numbers in a computer that is not exact, but is fast and efficient. It is a fundamental concept in numerical computing.
                    
                </p>
                <span class='hidden'>4</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/03/dirichlet-distribution-applications">
                            
                            Dirichlet Distribution and Its Applications
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        28 March 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    From latent Dirichlet allocation to Bayesian inference, and beyond, the Dirichlet distribution is a powerful tool in the data scientist's toolbox.
                    
                </p>
                <span class='hidden'>5</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/03/conjugate-priors">
                            
                            Conjugate Priors - Binomial Beta Pair
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        27 March 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Bayesian inference is almost 'everywhere' in data science; with the advance of computational power, it is now possible to apply Bayesian inference to high-dimensional data. In this post, we will discuss the conjugate priors for the binomial distribution.
                    
                </p>
                <span class='hidden'>6</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/03/Johnson-Lindenstrauss-lemma">
                            
                            The Johnson Lindenstrauss Lemma
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        25 March 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    In the era of AI, the Johnson Lindenstrauss lemma provides the mathematical foundation for many applications of machine learning and deep learning, such as ChatGPT.
                    
                </p>
                <span class='hidden'>7</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/03/locality-sensitive-hashing">
                            
                            Locality Sensitive Hashing (LSH)
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        15 March 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    LSH is recognized as a key breakthrough that has had great impact in many fields of computer science including computer vision, databases, information retrieval, machine learning, and signal processing.
                    
                </p>
                <span class='hidden'>8</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/03/approximating-distinct-elements">
                            
                            Approximating Distinct Element in a Stream
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        08 March 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    This post explains a probabilistic counting algorithm with which one can estimate the number of distinct elements in a large collection of data in a single pass.
                    
                </p>
                <span class='hidden'>9</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/02/probabilistic-thinking3">
                            
                            Develop Some Fluency in Probabilistic Thinking (Part III)
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        28 February 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                    
                </p>
                <span class='hidden'>10</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/02/probabilistic-thinking2">
                            
                            Develop Some Fluency in Probabilistic Thinking (Part II)
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        16 February 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                    
                </p>
                <span class='hidden'>11</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/02/probabilistic-thinking">
                            
                            Develop Some Fluency in Probabilistic Thinking (Part I)
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        12 February 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                    
                </p>
                <span class='hidden'>12</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/math/2023/01/probability-review">
                            
                            Probability Review
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        03 January 2023
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    From time to time, we need to review those definitions and basic concepts from probability field.
                    
                </p>
                <span class='hidden'>13</span>
            
        </div>
    </div>
</body>
</html>
