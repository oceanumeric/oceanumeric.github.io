<!DOCTYPE html>
<html lang="en">
<head>
    <html lang="en">
  <head>
    <title>
      Tag: machine-learning
    </title>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name='author' content='Michael Wang Fei'>
    <meta name='keywords' content='
          machine learning,
          statistical machine learning,
          bayesian inference,
          statistics,
          computational statistics,
          linear algebra,
          numerical linear algebra,
          statistical software,
          deep learning,
          computer science,
          probability,
          math,
          mathematics,
          probabilistic reasoning
      '>
      <meta name='keywords' content=''>
      <link rel="stylesheet" href="/css/blog.css">
      <link rel="stylesheet" href="/css/markdown.css">
      <link rel="stylesheet" href="/css/trac.css">
      <link rel="shortcut icon" type="image/png" href="/images/favicon.png">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
      <!-- The loading of KaTeX is deferred to speed up page rendering -->
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
      <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tag: machine-learning</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Tag: machine-learning" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://oceanumeric.github.io//math-tags/machine-learning.html" />
<meta property="og:url" content="https://oceanumeric.github.io//math-tags/machine-learning.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tag: machine-learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Tag: machine-learning","url":"https://oceanumeric.github.io//math-tags/machine-learning.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
</html>


</head>

<body>
    <!DOCTYPE html>
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/math'>Math</a></li>
        <li><a href='/tags'>Tags</a></li>
    </ul>
</div>
</html>

    <div id='blog' class='wrap'>
        <h1>Tag: machine-learning</h1>
        <div id='posts' class='section'>
            
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/05/maximum-entropy-distributions">
                                
                                Maximum Entropy Distributions
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            06 May 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The connection between entropy and probability distributions is really interesting. In this post, I will explore the connection between entropy and probability distributions, and how we can use this connection to derive the most likely probability distribution given some constraints.
                        
                    </p>
                    <span class='hidden'>1</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/05/variational-inference-2">
                                
                                Variational Inference (2)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            01 May 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Modern Bayesian statistics relies on models for which the posterior is not easy to compute and corresponding algorithms for approximating them. Variational inference is one of the most popular methods for approximating the posterior. In this post, we will introduce the basic idea of variational inference and its application to a simple example.
                        
                    </p>
                    <span class='hidden'>2</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/variational-inference-1">
                                
                                Variational Inference (1)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            29 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Modern Bayesian statistics relies on models for which the posterior is not easy to compute and corresponding algorithms for approximating them. Variational inference is one of the most popular methods for approximating the posterior. In this post, we will introduce the basic idea of variational inference and its application to a simple example.
                        
                    </p>
                    <span class='hidden'>3</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/metropolis-hastings-algorithm">
                                
                                Metropolis-Hastings Algorithm
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            13 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The Metropolis-Hastings algorithm is a Markov chain Monte Carlo (MCMC) algorithm that generates a sequence of random variables from a probability distribution from which direct sampling is difficult.
                        
                    </p>
                    <span class='hidden'>4</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/approximating-the-posterior">
                                
                                Approximating the Posterior
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            12 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        When we use Bayesian inference, we need to compute the posterior distribution. In this post, we will look at some methods for approximating the posterior distribution.
                        
                    </p>
                    <span class='hidden'>5</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/conjugate-families">
                                
                                Conjugate Families
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            11 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        When we build a model, we need to choose a prior distribution. If we choose a prior distribution from the same family as the posterior distribution, we can use the posterior distribution as the new prior distribution. This is called a conjugate prior. In this post, we will look at some of the most common conjugate priors.
                        
                    </p>
                    <span class='hidden'>6</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/beta-binomial-bayesian-model">
                                
                                The Beta-Binomial Bayesian Model
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            08 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        With more data generating day by day, I believe Bayesian statistics is the way to go. That's why I'm writing this series of posts on Bayesian statistics. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.
                        
                    </p>
                    <span class='hidden'>7</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/gradient-methods">
                                
                                Gradient Methods
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            04 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Gradient descent is one of the most popular optimization algorithms in machine learning. It can be used for both convex and non-convex optimization problems. In this post, we will learn about the key ideas behind gradient descent and how it can be used to solve optimization problems.
                        
                    </p>
                    <span class='hidden'>8</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/svd-to-pca">
                                
                                From SVD to PCA
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            03 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The applications of Singular Value Decomposition (SVD) are manifold. In this post, we will focus on the application of SVD to PCA, which is a great tool for dimensionality reduction.
                        
                    </p>
                    <span class='hidden'>9</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/QR-factorization">
                                
                                QR Factorization
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            02 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        A QR factorization is a factorization of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. This kind of decomposition is useful in solving linear least squares problems and in the eigendecomposition of a matrix, which shows the structure of the matrix in terms of its eigenvalues and eigenvectors.
                        
                    </p>
                    <span class='hidden'>10</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/solving-linear-systems">
                                
                                Solving Linear Systems
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            01 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Linear systems of equations are the bread and butter of numerical linear algebra. Solving them is at the core of many machine learning algorithms and engineering applications.
                        
                    </p>
                    <span class='hidden'>11</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/floating-point-arithmetic">
                                
                                Floating-Point Arithmetic
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            01 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Floating-Point arithmetic is a way of representing real numbers in a computer. It is a way of representing numbers in a computer that is not exact, but is fast and efficient. It is a fundamental concept in numerical computing.
                        
                    </p>
                    <span class='hidden'>12</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/dirichlet-distribution-applications">
                                
                                Dirichlet Distribution and Its Applications
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            28 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        From latent Dirichlet allocation to Bayesian inference, and beyond, the Dirichlet distribution is a powerful tool in the data scientist's toolbox.
                        
                    </p>
                    <span class='hidden'>13</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/conjugate-priors">
                                
                                Conjugate Priors - Binomial Beta Pair
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            27 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Bayesian inference is almost 'everywhere' in data science; with the advance of computational power, it is now possible to apply Bayesian inference to high-dimensional data. In this post, we will discuss the conjugate priors for the binomial distribution.
                        
                    </p>
                    <span class='hidden'>14</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/Johnson-Lindenstrauss-lemma">
                                
                                The Johnson Lindenstrauss Lemma
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            25 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        In the era of AI, the Johnson Lindenstrauss lemma provides the mathematical foundation for many applications of machine learning and deep learning, such as ChatGPT.
                        
                    </p>
                    <span class='hidden'>15</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/locality-sensitive-hashing">
                                
                                Locality Sensitive Hashing (LSH)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            15 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        LSH is recognized as a key breakthrough that has had great impact in many fields of computer science including computer vision, databases, information retrieval, machine learning, and signal processing.
                        
                    </p>
                    <span class='hidden'>16</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/approximating-distinct-elements">
                                
                                Approximating Distinct Element in a Stream
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            08 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        This post explains a probabilistic counting algorithm with which one can estimate the number of distinct elements in a large collection of data in a single pass.
                        
                    </p>
                    <span class='hidden'>17</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/02/probabilistic-thinking3">
                                
                                Develop Some Fluency in Probabilistic Thinking (Part III)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            28 February 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                        
                    </p>
                    <span class='hidden'>18</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/02/probabilistic-thinking2">
                                
                                Develop Some Fluency in Probabilistic Thinking (Part II)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            16 February 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                        
                    </p>
                    <span class='hidden'>19</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/02/probabilistic-thinking">
                                
                                Develop Some Fluency in Probabilistic Thinking (Part I)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            12 February 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The foundation of machine learning and data science is probability theory. In this post, we will develop some fluency in probabilistic thinking with different examples, which prepare data scientists well for the sexist job of the 21st century.
                        
                    </p>
                    <span class='hidden'>20</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/01/probability-review">
                                
                                Probability Review
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            03 January 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        From time to time, we need to review those definitions and basic concepts from probability field.
                        
                    </p>
                    <span class='hidden'>21</span>
                
            
        </div>
    </div>
</body>
</html>
