<!DOCTYPE html>
<html lang="en">
<head>
    <html lang="en">
  <head>
    <title>
      Tag: bayesian-statistics
    </title>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name='author' content='Michael Wang Fei'>
    <meta name='keywords' content='
          machine learning,
          statistical machine learning,
          bayesian inference,
          statistics,
          computational statistics,
          linear algebra,
          numerical linear algebra,
          statistical software,
          deep learning,
          computer science,
          probability,
          math,
          mathematics,
          probabilistic reasoning
      '>
      <meta name='keywords' content=''>
      <link rel="stylesheet" href="/css/blog.css">
      <link rel="stylesheet" href="/css/markdown.css">
      <link rel="stylesheet" href="/css/trac.css">
      <link rel="shortcut icon" type="image/png" href="/images/favicon.png">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
      <!-- The loading of KaTeX is deferred to speed up page rendering -->
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
      <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tag: bayesian-statistics</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Tag: bayesian-statistics" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://oceanumeric.github.io//math-tags/bayesian-statistics.html" />
<meta property="og:url" content="https://oceanumeric.github.io//math-tags/bayesian-statistics.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tag: bayesian-statistics" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Tag: bayesian-statistics","url":"https://oceanumeric.github.io//math-tags/bayesian-statistics.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
</html>


</head>

<body>
    <!DOCTYPE html>
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/math'>Math</a></li>
        <li><a href='/tags'>Tags</a></li>
    </ul>
</div>
</html>

    <div id='blog' class='wrap'>
        <h1>Tag: bayesian-statistics</h1>
        <div id='posts' class='section'>
            
            
                
            
                
            
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/05/maximum-entropy-distributions">
                                
                                Maximum Entropy Distributions
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            06 May 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The connection between entropy and probability distributions is really interesting. In this post, I will explore the connection between entropy and probability distributions, and how we can use this connection to derive the most likely probability distribution given some constraints.
                        
                    </p>
                    <span class='hidden'>4</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/05/variational-inference-2">
                                
                                Variational Inference (2)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            01 May 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Modern Bayesian statistics relies on models for which the posterior is not easy to compute and corresponding algorithms for approximating them. Variational inference is one of the most popular methods for approximating the posterior. In this post, we will introduce the basic idea of variational inference and its application to a simple example.
                        
                    </p>
                    <span class='hidden'>5</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/variational-inference-1">
                                
                                Variational Inference (1)
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            29 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Modern Bayesian statistics relies on models for which the posterior is not easy to compute and corresponding algorithms for approximating them. Variational inference is one of the most popular methods for approximating the posterior. In this post, we will introduce the basic idea of variational inference and its application to a simple example.
                        
                    </p>
                    <span class='hidden'>6</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/metropolis-hastings-algorithm">
                                
                                Metropolis-Hastings Algorithm
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            13 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        The Metropolis-Hastings algorithm is a Markov chain Monte Carlo (MCMC) algorithm that generates a sequence of random variables from a probability distribution from which direct sampling is difficult.
                        
                    </p>
                    <span class='hidden'>7</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/approximating-the-posterior">
                                
                                Approximating the Posterior
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            12 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        When we use Bayesian inference, we need to compute the posterior distribution. In this post, we will look at some methods for approximating the posterior distribution.
                        
                    </p>
                    <span class='hidden'>8</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/conjugate-families">
                                
                                Conjugate Families
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            11 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        When we build a model, we need to choose a prior distribution. If we choose a prior distribution from the same family as the posterior distribution, we can use the posterior distribution as the new prior distribution. This is called a conjugate prior. In this post, we will look at some of the most common conjugate priors.
                        
                    </p>
                    <span class='hidden'>9</span>
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/04/beta-binomial-bayesian-model">
                                
                                The Beta-Binomial Bayesian Model
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            08 April 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        With more data generating day by day, I believe Bayesian statistics is the way to go. That's why I'm writing this series of posts on Bayesian statistics. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.
                        
                    </p>
                    <span class='hidden'>10</span>
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
                    <div class='post-row'>
                        <p class='post-title'>
                            <a href="/math/2023/03/conjugate-priors">
                                
                                Conjugate Priors - Binomial Beta Pair
                                
                            </a>
                        </p>
                        <p class='post-date'>
                            27 March 2023
                        </p>
                    </div>
                    <p class='post-subtitle'>
                        
                        Bayesian inference is almost 'everywhere' in data science; with the advance of computational power, it is now possible to apply Bayesian inference to high-dimensional data. In this post, we will discuss the conjugate priors for the binomial distribution.
                        
                    </p>
                    <span class='hidden'>17</span>
                
            
                
            
                
            
                
            
                
            
                
            
                
            
                
            
        </div>
    </div>
</body>
</html>
