<!DOCTYPE html>
<html lang="en">
<head>
    <html lang="en">
  <head>
    <title>
      GPT in Review - Assessing Performance, Speed, and Cost
    </title>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name='author' content='Michael Wang Fei'>
    <meta name='keywords' content='
          machine learning,
          statistical machine learning,
          bayesian inference,
          statistics,
          computational statistics,
          linear algebra,
          numerical linear algebra,
          statistical software,
          deep learning,
          computer science,
          probability,
          math,
          mathematics,
          probabilistic reasoning
      '>
      <meta name='keywords' content='gpt, gpt-3, gpt-4, gpt-5, openai, nlp, natural language processing, transformer, bert, elmo, albert, t5, turing, turing test, performance, speed, cost, review, comparison, benchmark, evaluation, assessment, analysis, summary, conclusion'>
      <link rel="stylesheet" href="/css/blog.css">
      <link rel="stylesheet" href="/css/markdown.css">
      <link rel="stylesheet" href="/css/trac.css">
      <link rel="shortcut icon" type="image/png" href="/images/favicon.png">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
      <!-- The loading of KaTeX is deferred to speed up page rendering -->
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
      <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Vollkorn:ital,wght@0,400..900;1,400..900&display=swap" rel="stylesheet">
      <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>GPT in Review - Assessing Performance, Speed, and Cost</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="GPT in Review - Assessing Performance, Speed, and Cost" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Ever since the release of GPT-3 by OpenAI, the world has been buzzing with excitement about the capabilities of large language models. I have used API from OpenAI to solve various tasks and developed some applications, such as a cv job matching tool and a firm search and profile generator tool like perplexity search." />
<meta property="og:description" content="Ever since the release of GPT-3 by OpenAI, the world has been buzzing with excitement about the capabilities of large language models. I have used API from OpenAI to solve various tasks and developed some applications, such as a cv job matching tool and a firm search and profile generator tool like perplexity search." />
<link rel="canonical" href="https://oceanumeric.github.io//blog/gpt-in-review" />
<meta property="og:url" content="https://oceanumeric.github.io//blog/gpt-in-review" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-12T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="GPT in Review - Assessing Performance, Speed, and Cost" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-05-12T00:00:00+00:00","datePublished":"2024-05-12T00:00:00+00:00","description":"Ever since the release of GPT-3 by OpenAI, the world has been buzzing with excitement about the capabilities of large language models. I have used API from OpenAI to solve various tasks and developed some applications, such as a cv job matching tool and a firm search and profile generator tool like perplexity search.","headline":"GPT in Review - Assessing Performance, Speed, and Cost","mainEntityOfPage":{"@type":"WebPage","@id":"https://oceanumeric.github.io//blog/gpt-in-review"},"url":"https://oceanumeric.github.io//blog/gpt-in-review"}</script>
<!-- End Jekyll SEO tag -->

  </head>
</html>


</head>
<body>
<div class='content'>
    <!DOCTYPE html>
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/math'>Math</a></li>
        <li><a href='/tags'>Tags</a></li>
    </ul>
</div>
</html>

    <div class='front-matter'>
        <div class='wrap'>
            <h1>GPT in Review - Assessing Performance, Speed, and Cost</h1>
            <h4>A special review of the GPT models by OpenAI for certain tasks</h4>
            <div class='bylines'>
                <div class='byline'>
                    
                    
                        <span class="post-tags">
                            <i class="fa fa-tags"></i>
                            
                            <a href="/blog-tags/chatgpt">chatgpt</a>, 
                            
                            <a href="/blog-tags/gpt-3">gpt-3</a>, 
                            
                            <a href="/blog-tags/gpt-4">gpt-4</a>, 
                            
                            <a href="/blog-tags/openai">openai</a>, 
                            
                            <a href="/blog-tags/prompt-engineering">prompt-engineering</a>, 
                            
                            <a href="/blog-tags/performance">performance</a>, 
                            
                            <a href="/blog-tags/speed">speed</a>, 
                            
                            <a href="/blog-tags/cost">cost</a>, 
                            
                            <a href="/blog-tags/review">review</a>, 
                            
                            <a href="/blog-tags/evaluation">evaluation</a>
                            
                        </span>
                    
                    <h3>Published</h3>
                    <p>12 May 2024</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>Ever since the release of GPT-3 by OpenAI, the world has been buzzing with excitement about the capabilities of large language models. I have used API from OpenAI to solve various tasks and developed some applications, such as <a href="https://www.beprepared.studio/" target="_blank">a cv job matching tool</a> and <a href="https://www.hypergi.com/firm-search" target="_blank">a firm search and profile generator tool</a> like perplexity search.</p>

<p>I have heard many people told me that it is hard to scale the application with GPT models due to many reasons, such as the cost, the speed, and the performance. In this blog post, I will review the GPT models by OpenAI for certain tasks:</p>

<ul>
  <li><a href="#summarization-based-on-goolge-search-results">Summarization based on Goolge search results</a></li>
  <li><a href="#ontology-generation-for-a-specific-domain">Ontology generation for a specific domain</a></li>
  <li><a href="#name-entity-recognition-for-a-specific-domain">Name Entity Recognition for a specific domain</a></li>
</ul>

<h2 id="summarization-based-on-goolge-search-results">Summarization based on Goolge search results</h2>

<p>During the development of the firm search and profile generator tool, I have used GPT-3.5-turbo-0125 to summarize the search results from Google. The summarization task is to generate a short description of the search results. The input to the model is the search results from Google, and the output is the summary of the search results.</p>

<p>Figure 1 gives the plot of the query time of GPT models for different tokens. The query time increases with the number of tokens, and the heterogeneity of the query time is also observed across different tokens.</p>

<div class="figure">
    <img src="/blog/images/gpt-querytime-tokens.png" alt="euleriana_map" style="width: 87%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 1.</span> Query time of GPT-3.5-turbo-0125 for different tokens.
    </div>
</div>

<p>I am not a premium user of OpenAI, so I do not know whether the query time is the same for all users. Maybe for those who pay more or have a higher usage, the query time is faster. As you can see from the figure 1, the average query time is around 2-3 seconds for any task with more than 300 tokens. One cannot argue that this kind of query time is acceptable for many applications.</p>

<div class="figure">
    <img src="/blog/images/gpt-querycost-tokens.png" alt="euleriana_map" style="width: 87%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 2.</span> Query cost of GPT-3.5-turbo-0125 for different tokens.
    </div>
</div>

<p>The good news is that the cost of the query is not that high. As you can see from the figure 2, the cost of the query is around 0.001 USD for any task with more than 300 tokens. It is actually very cheap to use GPT models for many applications.</p>

<p>There is another thing that I want to mention. <strong>The function calling of the GPT models is not stable</strong>. Sometimes, you got repeated results, and sometimes, you got different formats of results. I do not know why this happens, but it is something that you need to be aware of when you use GPT models. For instance, I set up the format of the results to be a nested json object, but sometimes, GPT models could not return the results in the format that I set up.</p>

<p>I embedded my code including prompt and function calling in the following gist:</p>

<script src="https://gist.github.com/oceanumeric/fd777cbcfa31bbb4fb211ce02a4c3818.js"></script>

<h2 id="ontology-generation-for-a-specific-domain">Ontology generation for a specific domain</h2>

<p>This is the task that I personally found it very interesting to explore with GPT models. The ontology generation task is to generate a list of concepts and their relationships for a specific domain. The input to the model is the domain-specific text, and the output is the list of concepts and their relationships.</p>

<p>Overall, the GPT models amazed me with their performance on the ontology generation task, especially GPT-4 models. However, since this task is relatively hard, the query time is way longer than the summarization task. The average query time is 5-6 seconds for any task with more than 300 tokens, which is twice as long as the summarization task. Sometimes, the query time could be longer than 10 seconds, which is not acceptable for many applications. The following figure shows the plot of the query time of GPT models for different tokens comparing to the summarization task.</p>

<div class="figure">
    <img src="/blog/images/gpt-querytime-tasks.png" alt="euleriana_map" style="width: 87%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 3.</span> Query time of GPT-4-turbo-0125 for different tokens.
    </div>
</div>

<p>It is interesting to notice that ‘GPT modesl’ behaves like a human being, which means that the <em>‘thinking time is longer for harder tasks’</em>. For the harder tasks, we observe more heterogeneity in the query time across different tokens. The query time is more stable for the easier tasks, such as summarization.</p>

<h2 id="name-entity-recognition-for-a-specific-domain">Name Entity Recognition for a specific domain</h2>

<p>The last task that I want to review is the name entity recognition task. I did many experiments with GPT models on this task because one of my research projects is related to this task. The name entity recognition task is to recognize the names of entities in a specific domain. The input to the model is the news title, and the expected output is firm names, country names, and some properties of the entities, such as whether the firm is a public firm or a private firm. The following table gives some examples of the input and output of the name entity recognition task.</p>

<table>
  <thead>
    <tr>
      <th>News Title</th>
      <th>Company Name</th>
      <th>Location</th>
      <th>Is Company</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Deputy Director Hu Wei of the Institute of Hydrobiology visited Maotai Group for investigation and exchange</td>
      <td>Maotai Group</td>
      <td>China</td>
      <td>true</td>
    </tr>
    <tr>
      <td>Institute leaders visited Insect Technology Development Company</td>
      <td>Insect Technology Development Company</td>
      <td>China</td>
      <td>true</td>
    </tr>
    <tr>
      <td>Vice President Guo Ying of Sugon Group and his team visited Ganjiang Innovation Institute</td>
      <td>Sugon Group</td>
      <td>China</td>
      <td>true</td>
    </tr>
    <tr>
      <td>Delegation from GlaxoSmithKline visited Institute of Neuroscience</td>
      <td>GlaxoSmithKline</td>
      <td>Unknown</td>
      <td>false</td>
    </tr>
    <tr>
      <td>Hu Junpeng and his team from Angel Yeast Co., Ltd. visited Yarlong Academy of Sciences and exchanged views</td>
      <td>Angel Yeast Co., Ltd.</td>
      <td>China</td>
      <td>false</td>
    </tr>
  </tbody>
</table>

<p>The performance of GPT models on the name entity recognition task is truly amazing. The average F1 score is around 0.9 for any task with more than 300 tokens. For my research project, the acuracy of the name entity recognition task is around 0.92, which is very high. For some news titles in Chinese, the accuracy could be lower, which is around 0.89. It is still acceptable for many applications.</p>

<p>If you have tried other NLP libraries, such as spaCy, or other NER models, such as BERT, you will find that the performance of GPT models is much better than those models.</p>

<p>In conclusion, the GPT models by OpenAI are very powerful for many NLP tasks. The performance of the models is very high, and the cost of the models is very low. However, the speed of the models is not that fast, especially for harder tasks. The query time is around 2-3 seconds for the summarization task, 5-6 seconds for the ontology generation task, and 1-2 seconds for the name entity recognition task. The query time is more stable for the easier tasks, such as summarization, and more heterogeneity is observed in the query time across different tokens for the harder tasks.</p>

<p>I think the trade-off between the performance, the speed, and the cost will shapre the competition of the GPT models in the future. As we all know, there is no free lunch in the world. How to balance the trade-off is the key to the success of the GPT models.</p>

    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"></ol>
        </div>
    </div>
</div>
<!-- back-to-top button from Mkdocs material -->
<a
href="#"
id="back-top"
aria-label="Back-to-top link"
style="
position: fixed;
bottom: 10%;
margin-left:85%;
color: #808080;
background-color: #FFFFFF;"
hidden
>
<img width="30px" height="30px" alt="up-arrow" src="/images/up-arrow.png">
</a>

<script src="/assets/js/codeCopy.js"></script>
<script src="/assets/js/backTotop.js"></script>
<script>
    var lis = document.getElementsByClassName("footnotes")
    for (let i = 0; i < lis.length; i++){
        var li_tag = lis[i].getElementsByTagName('li')
    
        for (let j = 0; j < li_tag.length; j++) {
            li_tag[j].setAttribute('role', 'link')
        }
        var a_tag = lis[i].getElementsByTagName('a')
    
        for (let k = 0; k < a_tag.length; k++) {
            a_tag[k].setAttribute('role', 'link')
        }
    }
    </script>
    <style>
        .zoom-img{
            display: block;
            height: auto;
            transition: transform ease-in-out 0.7s;
            cursor: zoom-in;
        }
        .image-zoom-scale{
            transform: scale(1.7);
            cursor: zoom-out;
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
            z-index: 100;
            position: relative;
        }
    </style>
    <script>
        document.querySelectorAll('.zoom-img').forEach(item => {
        item.addEventListener('click', function () {
            this.classList.toggle('image-zoom-scale');
        })
        });
    </script>
</body>
</html>