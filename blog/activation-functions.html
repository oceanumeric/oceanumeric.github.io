<!DOCTYPE html>
<html lang="en">
<head>
    <html lang="en">
  <head>
    <title>
      Understanding Activation Functions in Neural Networks
    </title>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name='author' content='Michael Wang Fei'>
    <meta name='keywords' content='
          machine learning,
          statistical machine learning,
          bayesian inference,
          statistics,
          computational statistics,
          linear algebra,
          numerical linear algebra,
          statistical software,
          deep learning,
          computer science,
          probability,
          math,
          mathematics,
          probabilistic reasoning
      '>
      <meta name='keywords' content='deep learning, pytorch, data science, data collection, data processing, data analysis, model training'>
      <link rel="stylesheet" href="/css/blog.css">
      <link rel="stylesheet" href="/css/markdown.css">
      <link rel="stylesheet" href="/css/trac.css">
      <link rel="shortcut icon" type="image/png" href="/images/favicon.png">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
      <!-- The loading of KaTeX is deferred to speed up page rendering -->
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
      <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Understanding Activation Functions in Neural Networks</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Understanding Activation Functions in Neural Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="After the innovation like ChatGPT, I begun to think about the fundamentals of deep learning models. In the old days (I mean three to five years ago), I had some doubt on deep learning models as there is lack of theoretical support. Compared to traditional machine learning and statistcal models, deep learning models are more like black boxes. However, somehow it works and it is creating a lot of impact in the industry." />
<meta property="og:description" content="After the innovation like ChatGPT, I begun to think about the fundamentals of deep learning models. In the old days (I mean three to five years ago), I had some doubt on deep learning models as there is lack of theoretical support. Compared to traditional machine learning and statistcal models, deep learning models are more like black boxes. However, somehow it works and it is creating a lot of impact in the industry." />
<link rel="canonical" href="https://oceanumeric.github.io//blog/activation-functions" />
<meta property="og:url" content="https://oceanumeric.github.io//blog/activation-functions" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-14T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Understanding Activation Functions in Neural Networks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-04-14T00:00:00+00:00","datePublished":"2023-04-14T00:00:00+00:00","description":"After the innovation like ChatGPT, I begun to think about the fundamentals of deep learning models. In the old days (I mean three to five years ago), I had some doubt on deep learning models as there is lack of theoretical support. Compared to traditional machine learning and statistcal models, deep learning models are more like black boxes. However, somehow it works and it is creating a lot of impact in the industry.","headline":"Understanding Activation Functions in Neural Networks","mainEntityOfPage":{"@type":"WebPage","@id":"https://oceanumeric.github.io//blog/activation-functions"},"url":"https://oceanumeric.github.io//blog/activation-functions"}</script>
<!-- End Jekyll SEO tag -->

  </head>
</html>


</head>
<body>
<div class='content'>
    <!DOCTYPE html>
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/math'>Math</a></li>
        <li><a href='/tags'>Tags</a></li>
    </ul>
</div>
</html>

    <div class='front-matter'>
        <div class='wrap'>
            <h1>Understanding Activation Functions in Neural Networks</h1>
            <h4>As key components of neural networks, activation functions are responsible for transforming the input data into the desired output. In this article, we will discuss the most common activation functions and their applications.</h4>
            <div class='bylines'>
                <div class='byline'>
                    
                    
                        <span class="post-tags">
                            <i class="fa fa-tags"></i>
                            
                            <a href="/blog-tags/deep-learning">deep-learning</a>, 
                            
                            <a href="/blog-tags/pytorch">pytorch</a>, 
                            
                            <a href="/blog-tags/data-science">data-science</a>, 
                            
                            <a href="/blog-tags/python">python</a>
                            
                        </span>
                    
                    <h3>Published</h3>
                    <p>14 April 2023</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>After the innovation like ChatGPT, I begun to think about the fundamentals of deep learning models. In the old days (I mean three to five years ago), I had some doubt on deep learning models as there is lack of theoretical support. Compared to traditional machine learning and statistcal models, deep learning models are more like black boxes. However, somehow it works and it is creating a lot of impact in the industry.</p>

<p>Therefore, I updated my perception on deep learning models. Instread of caring a lot about the theoretical support, I think it is more important to understand the mechanism of deep learning models. It is better to think about the deep learning models as a nerual network architecture that could map data from one space to another space.</p>

<p>With this perspective, one could understand how two architectures have changed our world. One is the word2vec model, which maps words from one space to another space. The other is the transformer model, which maps sentences from one space to another space. As the famous paper stated, the attention is all we need.</p>

<p>In this article, I will talk about the activation functions. Since activation functions are the core of deep learning models, I think it is important to understand the mechanism of activation functions.</p>

<ul>
  <li><a href="#automatic-differentiation">Automatic differentiation</a></li>
  <li><a href="#activation-functions">Activation functions</a></li>
  <li><a href="#analyzing-the-effect-of-activation-functions">Analyzing the effect of activation functions</a></li>
  <li><a href="#code">Code</a></li>
</ul>

<h2 id="automatic-differentiation">Automatic differentiation</h2>

<p>Before we talk about activation functions, let’s talk about the automatic differentiation. In the old days, we need to manually calculate the gradient of the loss function with respect to the parameters. However, with the automatic differentiation, we could use the backpropagation algorithm to calculate the gradient of the loss function with respect to the parameters.</p>

<p>The automatic differentiation is the key to the success of deep learning models. With the automatic differentiation, we could train the deep learning models with the gradient descent algorithm. If you learn the concept of automatic differentiation in one framework, you could easily apply it to other frameworks. In this post, I will use the PyTorch framework to demonstrate the concept of automatic differentiation.</p>

<p>Let’s say we have a vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>, and the following two
functions:</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>x</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>y</mi><mn>1</mn></msub><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo>=</mo><mo stretchy="false">[</mo><mn>14</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>y</mi><mn>2</mn></msub><mo>=</mo><mi>x</mi><mo>⊙</mo><mi>x</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
x  = \begin{bmatrix} 0 \\ 1 \\ 2 \\ 3 \end{bmatrix}, \quad
y_1 = x^T x  = [14], \quad
y_2  = x \odot x = \begin{bmatrix}
0 \\ 1 \\ 4 \\ 9
\end{bmatrix}  \tag{1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mord">4</span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>

<p>The derivative of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span> with respect to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> is:</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>1</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mn>2</mn><mi>x</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>y</mi><mn>2</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mn>2</mn><mi>x</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\frac{\partial y_1}{\partial x} = 2x  = \begin{bmatrix} 0 \\ 2 \\ 4 \\ 6  \end{bmatrix}, \quad 
\frac{\partial y_2}{\partial x} = 2x = \begin{bmatrix} 0 \\ 4 \\ 4 \\ 6  \end{bmatrix} \tag{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault">x</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault">x</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>

<p>Although the two functions have the same derivative, <code class="language-plaintext highlighter-rouge">PyTorch</code> calculate the derivative in a different way as the first one is a scalar and the second one is a vector. The other thing to remember is that the <code class="language-plaintext highlighter-rouge">PyTorch</code> will accumulate the gradient of the loss function with respect to the parameters. Therefore, we need to set the gradient to zero before we calculate the gradient of the loss function with respect to the parameters.</p>

<p>During the training process, PyTorch will construct a graph to record the operations. The graph is called the computational graph. The computational graph is used to calculate the gradient of the loss function with respect to the parameters. The following figure illustrates the computational graph.</p>

<div class="figure">
    <img src="/images/blog/backward-grad-fig-1.png" alt="Pytorch autograd illustration" style="width: 60%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 1.</span> The computational graph.
    </div>
</div>

<p>With equations (1) and (2), we will show how to calculate the gradient in PyTorch. Three points need to be mentioned:</p>

<ol>
  <li>The <code class="language-plaintext highlighter-rouge">requires_grad_()</code> is set to <code class="language-plaintext highlighter-rouge">True</code> to record the operations in the computational graph.</li>
  <li>The gradients vary with the input data <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> and function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>, therefore,
    <ul>
      <li><code class="language-plaintext highlighter-rouge">.grad</code> is an attribute of the tensor.</li>
      <li><code class="language-plaintext highlighter-rouge">.grad</code> is None if the tensor does not require gradient.</li>
      <li><code class="language-plaintext highlighter-rouge">.grad</code> is accumulated if the tensor requires gradient.</li>
      <li><code class="language-plaintext highlighter-rouge">.grad</code> will be updated after the <code class="language-plaintext highlighter-rouge">backward()</code> function is called.</li>
      <li><code class="language-plaintext highlighter-rouge">.grad.zero_()</code> is used to set the gradient to zero if you want to calculate the gradient of a different loss function with respect to the same parameters manually.</li>
    </ul>
  </li>
  <li>The <code class="language-plaintext highlighter-rouge">backward()</code> function is used to calculate the gradient of the loss function with respect to the parameters.</li>
</ol>

<p>Here is the code will produce a <code class="language-plaintext highlighter-rouge">RuntimeError</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>  <span class="c1"># initialize tensor
</span><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># no gradient
# set gradient attribute
</span><span class="n">x</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># default is 
# equivalent to x.requires_grad = True
# or x = torch.arange(4.0, requires_grad=True)
</span><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># still None
</span><span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
<span class="n">y1</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y2</span><span class="p">)</span>
<span class="n">y2</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># grad can be implicitly created only for scalar outputs
</span></code></pre></div></div>

<p>Here is the code will produce the correct result:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># tensor([0., 1., 2., 3.], requires_grad=True)
</span><span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
<span class="c1"># tensor(14., grad_fn=&lt;DotBackward0&gt;)
</span><span class="n">y1</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># tensor([0., 2., 4., 6.])
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Without zeroing the gradient"</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">y2</span><span class="p">.</span><span class="nf">sum</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># tensor([ 0.,  4.,  8., 12.])
# you can see that the gradient is accumulated
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Setting the gradient to zero"</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># tensor([0., 0., 0., 0.])
</span><span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y2</span><span class="p">)</span>
<span class="c1"># tensor([0., 1., 4., 9.], grad_fn=&lt;MulBackward0&gt;)
</span><span class="n">y2</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># tensor([0., 2., 4., 6.])
</span></code></pre></div></div>

<p><em>Remark:</em> since the gradient does not depend on the value of the output <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>, that’s why the following two lines of code produce the same result:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># when the function is related to vector operations
</span><span class="n">y2</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">y2</span><span class="p">.</span><span class="nf">sum</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>One last thing you need to know that PyTorch has a function called <code class="language-plaintext highlighter-rouge">detach()</code> which will detach the tensor from the computational graph. Doing so could give us the gradient of the loss function with respect to the certain parameters in certain layers or steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">z</span><span class="p">.</span><span class="nf">sum</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">u</span>  <span class="c1"># tensor([True, True, True, True])
# refernce https://d2l.ai/chapter_preliminaries/autograd.html#detaching-computation
</span></code></pre></div></div>

<h2 id="activation-functions">Activation functions</h2>

<p>The activation functions are used to introduce non-linearity to the neural network. The following figure illustrates the activation functions.</p>

<div class="figure">
    <img src="/images/blog/activation-funs-1.png" alt="Pytorch autograd illustration" style="width: 80%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 2.</span> The visualization of the activation functions.
    </div>
</div>

<p>According to the figure 1, different activation functions give different non-linear behavior. The following figure illustrates the non-linear behavior of the activation functions. However, the key point is that the effect will be triggered when the input is large enough. And the gradient of the activation function will capture this effect.</p>

<h2 id="analyzing-the-effect-of-activation-functions">Analyzing the effect of activation functions</h2>

<p>After we have a basic understanding of the activation functions, we will analyze the effect of the activation functions. We will train a neural network with different activation functions and compare the results. The dataset we will use is the Fashion-MNIST dataset.</p>

<p>When we choose the activation function, we want to make sure that the
gradient will not vanish or explode. Since PyTorch has a built-in function, the chance of the gradient exploding is low. However, the gradient vanishing is still a problem. The following figure illustrates the gradient vanishing problem.</p>

<div class="figure">
    <img src="/images/blog/activation-funs-2.png" alt="Pytorch autograd illustration" class="zoom-img" style="width: 90%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 3.</span> The histogram of the gradient for different activation functions (you can zoom in to see the details).
    </div>
</div>

<p>The sigmoid activation function shows a clearly undesirable behavior. While the gradients for the output layer are very large with up to 0.1, the input layer has the lowest gradient norm across all activation functions with only 1e-5. If you look at the sigmoid function and its derivative in Figure 2, you can notice that the derivative becomes 
very small for a large positive or negative number. This causes the gradient to vanish. The ReLU activation function does not have this problem. The gradient is either 0 or 1.</p>

<p>Now, we will train neural networks with different activation functions and compare the results. The following table shows the results (accuracy is in percentage). The training time is measured by the time of total time it took until it reached to the saturation rate. The epoch of saturation is the epoch when the accuracy does not change much.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Activation function</th>
      <th>Training accuracy</th>
      <th>Test accuracy</th>
      <th>Training time (seconds)</th>
      <th>Epoch of saturation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Sigmod</td>
      <td>9.93</td>
      <td>10.19</td>
      <td>31.44</td>
      <td>5</td>
    </tr>
    <tr>
      <td style="text-align: left">Tanh</td>
      <td>86.22</td>
      <td>85.32</td>
      <td>60.93</td>
      <td>10</td>
    </tr>
    <tr>
      <td style="text-align: left">ReLU</td>
      <td>85.93</td>
      <td>84.84</td>
      <td>56.66</td>
      <td>9</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU</td>
      <td>85.71</td>
      <td>84.77</td>
      <td>56.26</td>
      <td>9</td>
    </tr>
    <tr>
      <td style="text-align: left">ELU</td>
      <td>84.52</td>
      <td>83.21</td>
      <td>37.48</td>
      <td>6</td>
    </tr>
    <tr>
      <td style="text-align: left">Swish</td>
      <td>85.54</td>
      <td>85.01</td>
      <td>80.36</td>
      <td>13</td>
    </tr>
  </tbody>
</table>

<p>As you can see that the sigmoid activation function has the lowest accuracy. The Tanh activation function has the highest accuracy. The ReLU activation function and the Leaky ReLU activation function have similar accuracy. The ELU activation function gives the relative good accuracy. However, it takes less time to train the neural network.</p>

<p>Except for the sigmoid activation function, the other activation functions have similar accuracy. The choice of activation function depends on many factors. But sigmoid function should not be the first choice unless you have a good reason to use it.</p>

<p>In figure 3, we visualized the distribution of the gradient. We could also visualize the distribution of the output of the activation functions. The gradient measures <em>the sensitivity of the output with respect to the input</em>. The output of the activation function across different layers could tell us how the input is transformed.</p>

<div class="figure">
    <img src="/images/blog/activation-funs-3.png" alt="Pytorch autograd illustration" class="zoom-img" style="width: 90%; display: block; margin: 0 auto;" />
    <div class="caption">
        <span class="caption-label">Figure 4.</span> The histogram of the output of each layer (you can zoom in to see the details).
    </div>
</div>

<p>As it is shown in the figure 3, the output of each layer for different neural network models is very different. However, the accuracy of the neural network is similar. This is very interesting. As all activation functions show slightly different behavior although obtaining similar performance for our simple network, it becomes apparent that the selection of the “optimal” activation function really depends on many factors, and is not the same for all possible networks.</p>

<h2 id="code">Code</h2>

<p>Here is the code for the neural network with different activation functions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># %%
# import packages that are not related to torch
</span><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">inspect</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="c1"># torch import
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.utils.data</span> <span class="k">as</span> <span class="n">tu_data</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">FashionMNIST</span>


<span class="c1"># region --------- environment setup --------- ###
# set up the data path
</span><span class="n">DATA_PATH</span> <span class="o">=</span> <span class="s">"../data"</span>
<span class="n">SAVE_PATH</span> <span class="o">=</span> <span class="s">"../pretrained/ac_fun"</span>


<span class="c1"># function for setting seed
</span><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>


<span class="c1"># set up seed globally and deterministically
</span><span class="nf">set_seed</span><span class="p">(</span><span class="mi">76</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># set up device
</span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="c1"># endregion
</span>

<span class="c1"># region --------- data prepocessing --------- ###
</span><span class="k">def</span> <span class="nf">_get_data</span><span class="p">():</span>
    <span class="s">"""
    download the dataset from FashionMNIST and transfom it to tensor
    """</span>
    <span class="c1"># set up the transformation
</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,))]</span>
    <span class="p">)</span>

    <span class="c1"># download the dataset
</span>    <span class="n">train_set</span> <span class="o">=</span> <span class="nc">FashionMNIST</span><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="s">"./data"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="nc">FashionMNIST</span><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="s">"./data"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span>


<span class="k">def</span> <span class="nf">_visualize_data</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">):</span>
    <span class="s">"""
    visualize the dataset by randomly sampling
    9 images from the dataset
    """</span>
    <span class="c1"># set up the figure
</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">col</span><span class="p">,</span> <span class="n">row</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span> <span class="o">*</span> <span class="n">row</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span>
        <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">.</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>


<span class="c1"># endregion
</span>

<span class="c1"># region --------- activation functions --------- ###
</span><span class="k">class</span> <span class="nc">AcFun</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AcFun</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span>
        <span class="n">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s">"name"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="p">}</span>


<span class="c1"># inherit from AcFun
</span><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="c1"># it is important to know that pytorch class
</span>    <span class="c1"># has __call__ function, which calls the forward automatically
</span>    <span class="c1"># therefore when you intialize the class, you are calling
</span>    <span class="c1"># the forward function directly
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">negative_slope</span> <span class="o">=</span> <span class="n">negative_slope</span>
        <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">"negative_slope"</span><span class="p">]</span> <span class="o">=</span> <span class="n">negative_slope</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">negative_slope</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ELU</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Swish</span><span class="p">(</span><span class="n">AcFun</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># function to get gradient of the activation function
</span><span class="k">def</span> <span class="nf">_get_grad</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="s">"""
    get the gradient of the activation function at x
    """</span>
    <span class="c1"># copy the input tensor and set requires_grad to True
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nf">ac_fun</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># slower version y.backward(torch.ones_like(y))
</span>    <span class="n">y</span><span class="p">.</span><span class="nf">sum</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># faster version
</span>    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span>


<span class="k">def</span> <span class="nf">_vis_grad</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">ax</span><span class="p">):</span>
    <span class="s">"""
    visualize the gradient of the activation function
    Input:
        ac_fun: activation function
        x: input tensor
        ax: matplotlib axis
    """</span>
    <span class="c1"># calculate the output
</span>    <span class="n">y</span> <span class="o">=</span> <span class="nf">ac_fun</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># get the gradient
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="nf">_get_grad</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># pass the data to cpu and convert to numpy
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">grad</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="c1"># plot the gradient
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">"k-"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"AcFun"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="s">"k--"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Gradient"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s">"bold"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">vis_ac_fun</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">):</span>
    <span class="s">"""
    visualize the activation function and its gradient
    """</span>

    <span class="c1"># get number of activation functions
</span>    <span class="n">num</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">)</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">num</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># initialize the input tensor
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

    <span class="c1"># set up the figure
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">columns</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ac_fun</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">.</span><span class="nf">values</span><span class="p">()):</span>
        <span class="nf">_vis_grad</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>


<span class="c1"># endregion
</span>

<span class="c1"># region --------- build up a neural network --------- ###
</span><span class="k">class</span> <span class="nc">BaseNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    A simple neural network to show the effect of activation functions
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">ac_fun</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="s">"""
        Inputs:
            ac_fun: activation function
            input_size: size of the input = 28*28
            num_class: number of classes = 10
            hidden_sizes: list of hidden layer sizes that specify the layer sizes
        """</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># create a list of layers
</span>        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_sizes</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">layers_sizes</span><span class="p">)):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers_sizes</span><span class="p">[</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">layers_sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">)</span>
        <span class="c1"># add the last layer
</span>        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layers_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_class</span><span class="p">))</span>
        <span class="c1"># create a sequential neural network
</span>        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>  <span class="c1"># * is used to unpack the list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">num_nets</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">layers_sizes</span><span class="p">)</span>

        <span class="c1"># set up the config dictionary
</span>        <span class="n">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"ac_fun"</span><span class="p">:</span> <span class="n">ac_fun</span><span class="p">.</span><span class="n">config</span><span class="p">,</span>
            <span class="s">"input_size"</span><span class="p">:</span> <span class="n">input_size</span><span class="p">,</span>
            <span class="s">"num_class"</span><span class="p">:</span> <span class="n">num_class</span><span class="p">,</span>
            <span class="s">"hidden_sizes"</span><span class="p">:</span> <span class="n">hidden_sizes</span><span class="p">,</span>
            <span class="s">"num_of_nets"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">num_nets</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># flatten the input as 1 by 784 vector
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_layer_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
        <span class="s">"""
        print the summary of the model
        input_size: the size of the input tensor
                    in the form of (batch_size, channel, height, width)
        note: using * to unpack the tuple
        """</span>
        <span class="c1"># generate a random input tensor
</span>        <span class="c1">#
</span>        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="o">*</span><span class="n">input_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="s">"output shape:</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>


<span class="c1"># endregion
</span>

<span class="c1"># region --------- visualize the distribution of gradient --------- ###
</span><span class="k">def</span> <span class="nf">_vis_grad_dist</span><span class="p">(</span><span class="n">neural_net</span><span class="p">,</span> <span class="n">training_dataset</span><span class="p">,</span> <span class="n">ac_fun_dict</span><span class="p">):</span>
    <span class="s">"""
    Input:
        nueral_net: neural network model with activation function
                    such as foo_net = BaseNet(ReLU())
        training_dataset: training dataset
        ac_fun_dict: dictionary of activation functions
    """</span>

    <span class="n">fig_rows</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">)</span>
    <span class="n">fig_cols</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># number of nets
</span>
    <span class="c1"># load the data from the training dataset
</span>    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">tu_data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">training_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># get the first batch of data
</span>    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
    <span class="c1"># push the data to the device
</span>    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">fig_rows</span><span class="p">,</span> <span class="n">fig_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">3.7</span> <span class="o">*</span> <span class="n">fig_cols</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">fig_rows</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">ac_key</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">):</span>
        <span class="nf">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="c1"># push model to device
</span>        <span class="n">ac_fun_net</span> <span class="o">=</span> <span class="nf">neural_net</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">[</span><span class="n">ac_key</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="c1"># change the model to evaluation mode
</span>        <span class="n">ac_fun_net</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="c1"># pass the data through the network and get gradient
</span>        <span class="n">ac_fun_net</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  <span class="c1"># set the gradient to zero
</span>        <span class="n">preds</span> <span class="o">=</span> <span class="nf">ac_fun_net</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="c1"># extract the gradient of the first layer
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ac_fun_net</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">()</span>
            <span class="k">if</span> <span class="s">"weight"</span> <span class="ow">in</span> <span class="n">name</span>
        <span class="p">}</span>
        <span class="n">ac_fun_net</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  <span class="c1"># set the gradient to zero
</span>
        <span class="k">for</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">gradients</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
            <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s">"C</span><span class="si">{</span><span class="n">row_idx</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">ac_key</span><span class="si">}</span><span class="s">:</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>


<span class="c1"># endregion
</span>

<span class="c1"># region ######  -------- function to train the model  -------- #######
</span><span class="k">def</span> <span class="nf">train_the_model</span><span class="p">(</span>
    <span class="n">network_model</span><span class="p">,</span> <span class="n">training_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">7</span>
<span class="p">):</span>
    <span class="s">"""
    Train the neural network model, we stop if the validation loss
    does not improve for a certain number of epochs (patience=7)
    Inputs:
        network_model: the neural network model
        training_dataset: the training dataset
        val_dataset: the validation dataset
        num_epochs: the number of epochs
        patience: the number of epochs to wait before early stopping
    Output:
        the trained model
    """</span>

    <span class="c1"># initialize the model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">network_model</span>

    <span class="n">ac_fun_name</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">"ac_fun"</span><span class="p">][</span><span class="s">"name"</span><span class="p">]</span>
    <span class="c1"># push the model to the device
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

    <span class="c1"># hyperparameters setting
</span>    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

    <span class="c1"># create the loader
</span>    <span class="n">training_loader</span> <span class="o">=</span> <span class="n">tu_data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">training_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
    <span class="n">validation_loader</span> <span class="o">=</span> <span class="n">tu_data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="c1"># define the loss function
</span>    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># define the optimizer
</span>    <span class="c1"># we are using stochastic gradient descent
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

    <span class="c1"># print out the model summary
</span>    <span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># loss tracker
</span>    <span class="n">loss_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># validation score tracker
</span>    <span class="n">val_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">best_val_score</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">epoch_count</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># count the number of epochs
</span>    <span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="c1"># begin training
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="c1"># set the model to training mode
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">correct_preds</span><span class="p">,</span> <span class="n">total_preds</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">training_loader</span><span class="p">:</span>
            <span class="c1"># push the data to the device
</span>            <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

            <span class="c1"># forward pass
</span>            <span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

            <span class="c1"># backward pass
</span>            <span class="c1"># zero the gradient
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="c1"># calculate the gradient
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="c1"># update the weights
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="c1"># calculate the accuracy
</span>            <span class="n">correct_preds</span> <span class="o">+=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">total_preds</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

        <span class="n">epoch_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># append the loss score
</span>        <span class="n">loss_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="c1"># calculate the training accuracy
</span>        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">correct_preds</span> <span class="o">/</span> <span class="n">total_preds</span>
        <span class="c1"># calculate the validation accuracy
</span>        <span class="n">val_acc</span> <span class="o">=</span> <span class="nf">test_the_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">)</span>
        <span class="n">val_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>

        <span class="c1"># print out the training and validation accuracy
</span>        <span class="nf">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">"### ----- Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s"> Training accuracy: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">*</span><span class="mf">100.0</span><span class="si">:</span><span class="mf">03.2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span>
        <span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"                    Validation accuracy: </span><span class="si">{</span><span class="n">val_acc</span><span class="o">*</span><span class="mf">100.0</span><span class="si">:</span><span class="mf">03.2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">val_scores</span><span class="p">[</span><span class="n">best_val_score</span><span class="p">]</span> <span class="ow">or</span> <span class="n">best_val_score</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">best_val_score</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># one could save the model here
</span>            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">SAVE_PATH</span> <span class="o">+</span> <span class="sa">f</span><span class="s">"/</span><span class="si">{</span><span class="n">ac_fun_name</span><span class="si">}</span><span class="s">.pt"</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"We have not improved for </span><span class="si">{</span><span class="n">epoch_count</span><span class="si">}</span><span class="s"> epochs, stopping..."</span><span class="p">)</span>
            <span class="n">time_end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Took </span><span class="si">{</span><span class="n">time_end</span> <span class="o">-</span> <span class="n">time_start</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds to train the model"</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="c1"># save the model
</span>    <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">SAVE_PATH</span> <span class="o">+</span> <span class="sa">f</span><span class="s">"/</span><span class="si">{</span><span class="n">ac_fun_name</span><span class="si">}</span><span class="s">.pt"</span><span class="p">)</span>
    <span class="n">time_end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Took </span><span class="si">{</span><span class="n">time_end</span> <span class="o">-</span> <span class="n">time_start</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds to train the model"</span><span class="p">)</span>

    <span class="c1"># plot the loss scores and validation scores
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">loss_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">loss_scores</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="s">"Loss"</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">val_scores</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="s">"Validation Accuracy"</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="s">"Loss and Validation Accuracy of LeNet-5 for Fashion MNIST"</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">test_the_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_data_loader</span><span class="p">):</span>
    <span class="s">"""
    Test the model on the validation dataset
    Input:
        model: the trained model
        val_data_loader: the validation data loader
    """</span>
    <span class="c1"># set the model to evaluation mode
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="n">correct_preds</span><span class="p">,</span> <span class="n">total_preds</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_data_loader</span><span class="p">:</span>
        <span class="c1"># push the data to the device
</span>        <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

        <span class="c1"># no need to calculate the gradient
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
            <span class="c1"># get the index of the max log-probability
</span>            <span class="c1"># output is [batch_size, 10]
</span>            <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="c1"># item() is used to get the value of a tensor
</span>            <span class="c1"># move the tensor to the cpu
</span>            <span class="n">correct_preds</span> <span class="o">+=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="nf">view_as</span><span class="p">(</span><span class="n">preds</span><span class="p">)).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">total_preds</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>

    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">correct_preds</span> <span class="o">/</span> <span class="n">total_preds</span>

    <span class="k">return</span> <span class="n">test_acc</span>


<span class="c1"># endregion
</span>

<span class="c1"># region --- visualize the output for each layer --------- #####
</span><span class="k">def</span> <span class="nf">_visualize_output</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">ac_fun_dict</span><span class="p">):</span>
    <span class="s">"""
    visualize the output for each layer based on pretrained model
    """</span>

    <span class="c1"># will only do this for three activation functions
</span>    <span class="n">models_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Sigmoid"</span><span class="p">,</span> <span class="s">"Tanh"</span><span class="p">,</span> <span class="s">"ReLU"</span><span class="p">]</span>

    <span class="c1"># initialize a dictionary to store the output of each layer
</span>    <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">ac_fun_name</span> <span class="ow">in</span> <span class="n">models_list</span><span class="p">:</span>
        <span class="c1"># load the data
</span>        <span class="n">data_loader</span> <span class="o">=</span> <span class="n">tu_data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
        <span class="c1"># load the model
</span>        <span class="n">ac_fun</span> <span class="o">=</span> <span class="n">ac_fun_dict</span><span class="p">[</span><span class="n">ac_fun_name</span><span class="p">]</span>
        <span class="n">nn_model</span> <span class="o">=</span> <span class="nc">BaseNet</span><span class="p">(</span><span class="n">ac_fun</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">saved_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">SAVE_PATH</span> <span class="o">+</span> <span class="sa">f</span><span class="s">"/</span><span class="si">{</span><span class="n">ac_fun_name</span><span class="si">}</span><span class="s">.pt"</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">nn_model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">saved_model</span><span class="p">)</span>

        <span class="c1"># evaluate the model
</span>        <span class="n">nn_model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">nn_model</span><span class="p">.</span><span class="n">net</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">imgs</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
                <span class="n">layer_name</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span>
                <span class="n">output_dict_key</span> <span class="o">=</span> <span class="n">ac_fun_name</span> <span class="o">+</span> <span class="s">"_"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">)</span> <span class="o">+</span> <span class="s">"_"</span> <span class="o">+</span> <span class="n">layer_name</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="n">output_dict_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="n">fig_rows</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">models_list</span><span class="p">)</span>
    <span class="n">fig_cols</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">fig_rows</span><span class="p">,</span> <span class="n">fig_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_cols</span> <span class="o">*</span> <span class="mf">3.7</span><span class="p">,</span> <span class="n">fig_rows</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>

    <span class="n">color_map</span> <span class="o">=</span> <span class="p">{</span><span class="s">"Sigmoid"</span><span class="p">:</span> <span class="s">"C0"</span><span class="p">,</span> <span class="s">"Tanh"</span><span class="p">:</span> <span class="s">"C1"</span><span class="p">,</span> <span class="s">"ReLU"</span><span class="p">:</span> <span class="s">"C2"</span><span class="p">}</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">output_key</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">output_dict</span><span class="p">):</span>
        <span class="c1"># get the output
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="n">output_key</span><span class="p">]</span>
        <span class="c1"># get the activation function name
</span>        <span class="n">ac_fun_name</span> <span class="o">=</span> <span class="n">output_key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">"_"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># get the layer index
</span>        <span class="n">layer_idx</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">output_key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">"_"</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">layer_name</span> <span class="o">=</span> <span class="n">output_key</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">"_"</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
        <span class="c1"># get the axis
</span>        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
            <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">stat</span><span class="o">=</span><span class="s">"density"</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">color_map</span><span class="p">[</span><span class="n">ac_fun_name</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">ac_fun_name</span><span class="si">}</span><span class="s"> - Layer </span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>


<span class="c1"># endregion
</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Using torch"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Using device"</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
    <span class="c1"># set up finger config
</span>    <span class="c1"># it is only used for interactive mode
</span>    <span class="c1"># %config InlineBackend.figure_format = "retina"
</span>
    <span class="c1"># download the dataset
</span>    <span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="nf">_get_data</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Train set size:"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Test set size:"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"The shape of the image:"</span><span class="p">,</span> <span class="n">train_set</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"The size of vectorized image:"</span><span class="p">,</span> <span class="n">train_set</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># _visualize_data(train_set, test_set)
</span>
    <span class="c1"># create a dictionary of activation functions 6 activation functions
</span>    <span class="n">ac_fun_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"Sigmoid"</span><span class="p">:</span> <span class="nc">Sigmoid</span><span class="p">(),</span>
        <span class="s">"Tanh"</span><span class="p">:</span> <span class="nc">Tanh</span><span class="p">(),</span>
        <span class="s">"ReLU"</span><span class="p">:</span> <span class="nc">ReLU</span><span class="p">(),</span>
        <span class="s">"LeakyReLU"</span><span class="p">:</span> <span class="nc">LeakyReLU</span><span class="p">(),</span>
        <span class="s">"ELU"</span><span class="p">:</span> <span class="nc">ELU</span><span class="p">(),</span>
        <span class="s">"Swish"</span><span class="p">:</span> <span class="nc">Swish</span><span class="p">(),</span>
    <span class="p">}</span>
    <span class="c1"># set seaborn style
</span>    <span class="c1"># sns.set_style("ticks")
</span>    <span class="c1"># vis_ac_fun(ac_fun_dict)
</span>
    <span class="c1"># set seed
</span>    <span class="nf">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="c1"># check layer summary
</span>    <span class="n">foo</span> <span class="o">=</span> <span class="nc">BaseNet</span><span class="p">(</span><span class="n">ac_fun_dict</span><span class="p">[</span><span class="s">"Tanh"</span><span class="p">])</span>
    <span class="c1"># print(foo.config)
</span>    <span class="c1"># good habit to check the dimension dynamically in the network
</span>    <span class="n">foo</span><span class="p">.</span><span class="nf">_layer_summary</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>
    <span class="c1"># foo._layer_summary((28*28, 1)) will raise an error
</span>    <span class="c1"># for param_name, param_val in foo.named_parameters():
</span>    <span class="c1">#     print(param_name, param_val.shape, param_val.grad)
</span>
    <span class="c1"># split the dataset into train and validation
</span>    <span class="n">train_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">))</span>
    <span class="n">val_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">tu_data</span><span class="p">.</span><span class="nf">random_split</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">])</span>
    <span class="c1"># train_the_model(foo, train_dataset, val_dataset)
</span>    <span class="c1"># _vis_grad_dist(BaseNet, train_dataset, ac_fun_dict)
</span>    <span class="nf">_visualize_output</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">ac_fun_dict</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="references">References</h2>

<ol>
  <li><a href="https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/" target="_blank">How Computational Graphs are Executed in PyTorch</a></li>
</ol>

    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"></ol>
        </div>
    </div>
</div>
<!-- back-to-top button from Mkdocs material -->
<a
href="#"
id="back-top"
aria-label="Back-to-top link"
style="
position: fixed;
bottom: 10%;
margin-left:85%;
color: #808080;
background-color: #FFFFFF;"
hidden
>
<img width="30px" height="30px" alt="up-arrow" src="/images/up-arrow.png">
</a>

<script src="/assets/js/codeCopy.js"></script>
<script src="/assets/js/backTotop.js"></script>
<script>
    var lis = document.getElementsByClassName("footnotes")
    for (let i = 0; i < lis.length; i++){
        var li_tag = lis[i].getElementsByTagName('li')
    
        for (let j = 0; j < li_tag.length; j++) {
            li_tag[j].setAttribute('role', 'link')
        }
        var a_tag = lis[i].getElementsByTagName('a')
    
        for (let k = 0; k < a_tag.length; k++) {
            a_tag[k].setAttribute('role', 'link')
        }
    }
    </script>
    <style>
        .zoom-img{
            display: block;
            height: auto;
            transition: transform ease-in-out 0.7s;
            cursor: zoom-in;
        }
        .image-zoom-scale{
            transform: scale(1.7);
            cursor: zoom-out;
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
            z-index: 100;
            position: relative;
        }
    </style>
    <script>
        document.querySelectorAll('.zoom-img').forEach(item => {
        item.addEventListener('click', function () {
            this.classList.toggle('image-zoom-scale');
        })
        });
    </script>
</body>
</html>