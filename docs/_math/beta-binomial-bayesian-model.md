---
title: The Beta-Binomial Bayesian Model
subtitle: With more data generating day by day, I believe Bayesian statistics is the way to go. That's why I'm writing this series of posts on Bayesian statistics. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.
layout: math_page_template
date: 2023-04-08
keywords: probabilistic-thinking dirichlet-distribution text-mining machine-learning bayesian-inference bayesian-statistics 
published: true
tags: probability algorithm data-science machine-learning binomial-distribution bayesian-statistics beta-distribution conjugate-prior
---


In one of previous posts, I introduced the Beta-Binomial Bayesian model and its application in sports recruiting and text mining (Latent Dirichlet Allocation). I am very impressed by the power of this model. It is so simple and yet so powerful. It is also very easy to implement. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.

I strongly believe that Bayesian statistics will play a much bigger role in the future. That's why I am investing a lot of time in learning Bayesian statistics. I am also writing this series of posts on Bayesian statistics. I hope that this series of posts will help you to learn Bayesian statistics as well.

## Philosophy of Bayesian Statistics

{% katexmm %}

Almost everyone learns linear regression models in their statistics or machine learning or business analytics courses. From estimation to inference, much of the theory and practice of linear regression rely on Maximum Likelihood Estimation (MLE) and Law of Large Numbers (LLN). In other words, we assume that the data are generated by a fixed distribution. We then estimate the parameters of the distribution by maximizing the likelihood function. We also assume that the sample size is large enough so that the sample mean is close to the population mean. This is the frequentist philosophy.

However, in the real world, we often do not know the distribution of the data. We also do not know the sample size. Very often we only have a small sample. Or even we have a big sample, but the size of sub-samples decreases as the data becomes heterogeneous (this happens very often because of the complexity of the real world). In these cases, we cannot use the frequentist philosophy. We need to use the Bayesian philosophy.

If you want to have a deep understanding of Bayesian statistics, I highly recommend you to read my previous post  - [Conjugate Priors - Binomial Beta Pair](https://oceanumeric.github.io/math/2023/03/conjugate-priors){:target="_blank"}. I think the Latent Dirichlet Allocation (LDA) model is a good example of the Bayesian philosophy, which shows the power of the Bayesian philosophy.


Before we start, let's set up the notations we will use in this post:

- random variable: $X$
- probability density function: $f(x)$
- probability in 'idea world': $p$
- probability in 'real life': $\pi$
- hyperparameter: $\alpha, \beta, \cdots$


## The Beta distribution



Let $X$ be a random variable that takes values in the interval $[0,1]$. The Beta distribution is a continuous probability distribution with two parameters $\alpha$ and $\beta$. The probability density function of the Beta distribution is given by:

$$
f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)} \quad \text{for } x \in [0,1]
$$

where $B(\alpha, \beta)$ is the beta function:  

$$
B(\alpha, \beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
$$

It is easy to verify we have $f(x) = 1$ when $\alpha = \beta = 1$. The Beta distribution is symmetric when $\alpha = \beta$. The Beta distribution is skewed to the left when $\alpha > \beta$. The Beta distribution is skewed to the right when $\alpha < \beta$.

<div class='figure'>
    <img src="/math/images/beta_dist.png"
         alt="Inequality bounds compare"
         style="width: 70%; display: block; margin: 0 auto;"/>
    <div class='caption'>
        <span class='caption-label'>Figure 1.</span> The plot of the beta distribution with different values of $\alpha$ and $\beta$.
    </div>
</div>

_Remark:_ The Beta distribution is symmetric when $\alpha = \beta$. 

Now, let's list some properties of the Beta distribution. 

$$
\begin{aligned}
\text{E}[X] &= \int x \cdot f(x) dx =  \frac{\alpha}{\alpha + \beta} \\
\text{Var}[X] &= \int (x - \mathrm{E}(x))^2 \cdot  f(x) dx =  \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} \\
\text{Mode}[X] &= \argmax_x f(x) =  \frac{\alpha - 1}{\alpha + \beta - 2} 
\end{aligned}
$$

It is important to know that $\alpha, \beta$ could be any real numbers with $\alpha > 0$ and $\beta > 0$. 


<div class='figure'>
    <img src="/math/images/beta_distribution.png"
         alt="Inequality bounds compare"
         style="width: 70%; display: block; margin: 0 auto;"/>
    <div class='caption'>
        <span class='caption-label'>Figure 2.</span> The plot of the beta distribution. 
    </div>
</div>


## A simple example

Let's consider a simple example. Suppose you are basketball coach. Now, you are recruiting a new player. You ask this player to shoot 10 free throws. Assume that the player's probability of making a free throw is $p$. We can model this process as a Binomial distribution with $n = 10$ and $p$.

$$
X \sim \text{Binomial}(n=10, p) = \binom{10}{x} p^x (1-p)^{10-x}
$$

Now, let's assume three different scenarios:

- Scenario 1: this player is a rookie, which means his probability of making a free throw is $p = 0.5$.
- Scenario 2: this player is a veteran, which means his probability of making a free throw is $p = 0.7$.
- Scenario 3: this player is a superstar, which means his probability of making a free throw is $p = 0.9$.

We plot the distribution of $X$ in the three scenarios in Figure 3.

<div class='figure'>
    <img src="/math/images/free_throws.png"
         alt="Inequality bounds compare"
         style="width: 70%; display: block; margin: 0 auto;"/>
    <div class='caption'>
        <span class='caption-label'>Figure 3.</span> The plot of binomial distribution with different values of $p$.
    </div>
</div>

Now, image that we ask this player to shoot 10 free throws. We observe that he makes 6 free throws. Can you tell which scenario is more likely? From figure 3, we can make

- Scenario 1: $p = 0.5$, $X = 6$, $f(x) = 0.205$
- Scenario 2: $p = 0.7$, $X = 6$, $f(x) = 0.200$
- Scenario 3: $p = 0.9$, $X = 6$, $f(x) = 0.011$

This means the likelihood of Scenario 3 is much smaller than the likelihood of Scenario 1 and Scenario 2. Therefore, we can conclude that the player is more likely to be a rookie or a veteran.

But, how confident are we about this conclusion? Right now, we only have one observation. We do not know the distribution of $X$. We do not know the sample size. We do not know the true value of $p$. We only know that the player is more likely to be a rookie or a veteran.

There are two ways to solve this problem. The first way is to use frequentist statistics. The second way is to use Bayesian statistics. 

With frequentist statistics, we can ask our friend to shoot 10 free throws again and again (let's say 5 round). Then, we can calculate the average of likelihood and make a conclusion.

With Bayesian statistics, we can use the Beta distribution to model the distribution of $p$. Then, we can use the posterior distribution to make a conclusion.

When should one use frequentist statistics and when should one use Bayesian statistics? The answer is that it depends on the problem. In this example, we can use either frequentist statistics or Bayesian statistics.

But for many problems, we can only use Bayesian statistics. For example, if we want to estimate the probability of a person having a disease, we can only use Bayesian statistics. We cannot use frequentist statistics because we do not have a large sample size.

You get the idea. Bayesian statistics is more flexible than frequentist statistics. But, Bayesian statistics is also more complicated than frequentist statistics, which needs a prior distribution and a posterior distribution.

### Tune the hyperparameters for prior distribution













{% endkatexmm %}