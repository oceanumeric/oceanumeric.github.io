---
title: The Beta-Binomial Bayesian Model
subtitle: With more data generating day by day, I believe Bayesian statistics is the way to go. That's why I'm writing this series of posts on Bayesian statistics. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.
layout: math_page_template
date: 2023-04-08
keywords: probabilistic-thinking dirichlet-distribution text-mining machine-learning bayesian-inference bayesian-statistics 
published: true
tags: probability algorithm data-science machine-learning binomial-distribution bayesian-statistics beta-distribution conjugate-prior
---


In one of previous posts, I introduced the Beta-Binomial Bayesian model and its application in sports recruiting and text mining (Latent Dirichlet Allocation). I am very impressed by the power of this model. It is so simple and yet so powerful. It is also very easy to implement. In this post, I'll introduce the Beta-Binomial Bayesian model again. I'll also show how two communities (Python and R) have implemented this model.

I strongly believe that Bayesian statistics will play a much bigger role in the future. That's why I am investing a lot of time in learning Bayesian statistics. I am also writing this series of posts on Bayesian statistics. I hope that this series of posts will help you to learn Bayesian statistics as well.

## Philosophy of Bayesian Statistics

{% katexmm %}

Almost everyone learns linear regression models in their statistics or machine learning or business analytics courses. From estimation to inference, much of the theory and practice of linear regression rely on Maximum Likelihood Estimation (MLE) and Law of Large Numbers (LLN). In other words, we assume that the data are generated by a fixed distribution. We then estimate the parameters of the distribution by maximizing the likelihood function. We also assume that the sample size is large enough so that the sample mean is close to the population mean. This is the frequentist philosophy.

However, in the real world, we often do not know the distribution of the data. We also do not know the sample size. Very often we only have a small sample. Or even we have a big sample, but the size of sub-samples decreases as the data becomes heterogeneous (this happens very often because of the complexity of the real world). In these cases, we cannot use the frequentist philosophy. We need to use the Bayesian philosophy.

If you want to have a deep understanding of Bayesian statistics, I highly recommend you to read my previous post  - [Conjugate Priors - Binomial Beta Pair](https://oceanumeric.github.io/math/2023/03/conjugate-priors){:target="_blank"}. I think the Latent Dirichlet Allocation (LDA) model is a good example of the Bayesian philosophy, which shows the power of the Bayesian philosophy.


Before we start, let's set up the notations we will use in this post:

- random variable: $X$
- probability density function: $f(x)$
- probability in 'idea world': $p$
- probability in 'real life': $\pi$
- hyperparameter: $\alpha, \beta, \cdots$


## The Beta distribution



Let $X$ be a random variable that takes values in the interval $[0,1]$. The Beta distribution is a continuous probability distribution with two parameters $\alpha$ and $\beta$. The probability density function of the Beta distribution is given by:

$$
f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)} \quad \text{for } x \in [0,1]
$$

where $B(\alpha, \beta)$ is the beta function:  

$$
B(\alpha, \beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
$$

It is easy to verify we have $f(x) = 1$ when $\alpha = \beta = 1$. The Beta distribution is symmetric when $\alpha = \beta$. The Beta distribution is skewed to the left when $\alpha > \beta$. The Beta distribution is skewed to the right when $\alpha < \beta$.












{% endkatexmm %}